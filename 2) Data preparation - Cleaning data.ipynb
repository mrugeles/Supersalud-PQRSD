{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import data_utils\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('datasets/dataset_cie10.csv')\n",
    "display(dataset.head())\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIE 10 fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_year(value_range):\n",
    "    if(value_range == 'no_cie10'):\n",
    "        return -1\n",
    "    \n",
    "    num = int(value_range[:3])\n",
    "    unit = value_range[-1]\n",
    "    \n",
    "    map_unit = {\n",
    "        'A': num,\n",
    "        'M': num / 12,\n",
    "        'D': num / 365,\n",
    "        'H': num / 8760\n",
    "    }\n",
    "    return int(map_unit[unit])\n",
    "\n",
    "def get_edad_inf(value):\n",
    "    value_range = {\n",
    "        '999': -1,\n",
    "        'de 0 a 5 años': 0,\n",
    "        'de 13 a 17 años': 13,\n",
    "        'de 18 a 24 años': 18,\n",
    "        'de 25 a 29 años': 25,\n",
    "        'de 30 a 37 años': 30,\n",
    "        'de 38 a 49 años': 38,\n",
    "        'de 50 a 62 años': 50,\n",
    "        'de 6 a 12 años': 6,\n",
    "        'mayor de 63 años': 63\n",
    "            }\n",
    "    return value_range[value]\n",
    "\n",
    "def get_edad_sup(value):\n",
    "    value_range = {\n",
    "        '999': -1,\n",
    "        'de 0 a 5 años': 5,\n",
    "        'de 13 a 17 años': 17,\n",
    "        'de 18 a 24 años': 24,\n",
    "        'de 25 a 29 años': 29,\n",
    "        'de 30 a 37 años': 37,\n",
    "        'de 38 a 49 años': 49,\n",
    "        'de 50 a 62 años': 62,\n",
    "        'de 6 a 12 años': 12,\n",
    "        'mayor de 63 años': 120\n",
    "            }\n",
    "    return value_range[value]\n",
    "\n",
    "def in_range(row):\n",
    "    return row['AFEC_EDADR_INF'] >= row['LIMITE_INFERIOR_EDAD_Y'] and  row['AFEC_EDADR_SUP'] < row['LIMITE_SUPERIOR_EDAD_Y']\n",
    "\n",
    "def cie10_sexo(value):\n",
    "    if(value == 1 or value == 2):\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cie10_columns = [\n",
    "    'CAPITULO', \n",
    "    'NOMBRE_CAPITULO', \n",
    "    'COD_CIE_10_03', \n",
    "    'DESCRIPCION_COD_CIE_10_03', \n",
    "    'COD_CIE_10_04', \n",
    "    'DESCRIPCION_COD_CIE_10_04', \n",
    "    'SEXO', \n",
    "    'LIMITE_INFERIOR_EDAD', \n",
    "    'LIMITE_SUPERIOR_EDAD']\n",
    "\n",
    "dataset[cie10_columns] = dataset[cie10_columns].replace(np.nan, 'no_cie10', regex=True)\n",
    "dataset = dataset[dataset['NOMBRE_CAPITULO'] != 'no_cie10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in dataset.columns:\n",
    "    dataset[column] = dataset[column].apply(lambda value: '999' if value == 0 or value == '0' or value == 'nan' else value)     \n",
    "\n",
    "dataset = dataset[dataset['RIESGO_VIDA'] != '999']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['CIE10_SEXO'] = dataset['SEXO'].apply(cie10_sexo)\n",
    "dataset['LIMITE_INFERIOR_EDAD_Y'] = dataset['LIMITE_INFERIOR_EDAD'].apply(to_year)\n",
    "dataset['LIMITE_SUPERIOR_EDAD_Y'] = dataset['LIMITE_SUPERIOR_EDAD'].apply(to_year)\n",
    "dataset['AFEC_EDADR_INF'] = dataset['AFEC_EDADR'].apply(get_edad_inf)\n",
    "dataset['AFEC_EDADR_SUP'] = dataset['AFEC_EDADR'].apply(get_edad_sup)\n",
    "dataset['CIE10_RANGO_EDAD'] = dataset.apply(in_range, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special case: AFEC_DPTO\n",
    "\n",
    "States names can take similar names for a same state, and given that there are relatively few states, it's possible to manually fix these values to avoid duplicates in classe values.\n",
    "\n",
    "Some features are writen in different ways, for example, 'ARCHIPIELAGO DE SAN ANDRES, PROVIDENCIA Y SANTA CATALINA', 'SAN ANDRES' and 'SAN ANDRÉS' are the same state. Same for 'BOGOTA D.C' and 'BOGOTA D.C.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['AFEC_DPTO']].drop_duplicates().sort_values(by=['AFEC_DPTO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = data_utils.clean_afec_dpto(dataset)\n",
    "\n",
    "dataset[['AFEC_DPTO']].drop_duplicates().sort_values(by=['AFEC_DPTO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set(dataset['ENT_COD_DEPTO'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RIESGO_VIDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "riesgo_vida = dataset['RIESGO_VIDA'].value_counts()\n",
    "riesgo_vida.plot(kind='bar', title='Patients with life at risk.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove rows with missing info in our target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = data_utils.clean_riesgo_vida(dataset)\n",
    "\n",
    "riesgo_vida = dataset['RIESGO_VIDA'].value_counts()\n",
    "riesgo_vida.plot(kind='bar', title='Patients with life at risk.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIE_10\n",
    "\n",
    "In 'Data understanding' notebook we see CIE_10 was way too many missing values. '0' value is the most common value in the column so is not a good candidate for imputing values. But as the column contains descriptions about the patient's illness, we want to keep it as it can provide a signal to predict if the patient's life is at risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = data_utils.clean_cie_10(dataset)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riesgo_vida = dataset['RIESGO_VIDA'].value_counts()\n",
    "riesgo_vida.plot(kind='bar', title='Patients with life at risk.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing records with CIE_10 = 0 reduces drastically the dataset from 2'375.371 to 281.311 records but it provided a huge improvement in the target's balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing fields\n",
    "\n",
    "Acording to the oficial documentation, fields \"IDRANGOEDADES\", \"ID_MES\" and \"PQR_GRUPOALERTA\" have not statistical use, so they are removed from the dataset.\n",
    "\n",
    "Feature \"PQR_ESTADO\" has a significant statistical value that may bias the model. Once a PQRS enters the system, it goes through a series of states before the case is closed. First, Historycally, patients with life at risk can have a tendency to have a certain state or a relationship with and another feature (i.e patient's with life at risk may have most of their states as closed as they may have priority over other cases), so including \"PQR_ESTADO\" will make the model to make predictions over a feature that will not be statiastic relevant when introducing a new PQRS (When a new PQRS enters the system it will have a default state that is very unlikely to have the final state from the original data set).\n",
    "\n",
    "### Redundant features\n",
    "These features represent the same data, so we can keep only the codes and loose the descripion.\n",
    "\n",
    "* COD_MACROMOT = MACROMOTIVO\n",
    "* COD_MOTGEN = MOTIVO_GENERAL\n",
    "* COD_MOTESP = MOTIVO_ESPECIFICO\n",
    "* ENT_COD_DEPTO = ENT_DPTO\n",
    "* ENT_COD_MPIO = ENT_MPIO\n",
    "* PET_COD_DEPTO = PET_DPTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = data_utils.remove_features(dataset)\n",
    "\n",
    "display(dataset.head(n = 5))\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing features with more of 75% with empty data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_missing_cols = set(dataset.columns[dataset.eq('999').mean() > 0.75])\n",
    "dataset = dataset.drop(most_missing_cols, axis = 1)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Values: Adding empty mark columns for features with emty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(dataset['ENT_AMBITOIVC'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['ENT_AMBITOIVC'] = dataset['ENT_AMBITOIVC'].fillna('999')\n",
    "dataset['ENT_AMBITOIVC'] = dataset['ENT_AMBITOIVC'].apply(lambda value: '999' if value == 'nan' else value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dataset['IDPATOLOGIA_2'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['IDPATOLOGIA_2'] = dataset['IDPATOLOGIA_2'].fillna('999')\n",
    "dataset['IDPATOLOGIA_2'] = dataset['IDPATOLOGIA_2'].apply(lambda value: '999' if value == 'nan' or value == None else value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values = '999', strategy=\"most_frequent\")\n",
    "\n",
    "zero_values = set(dataset.columns[dataset.eq('999').mean() > 0])\n",
    "for feature in zero_values:\n",
    "    dataset[f'{feature}_is_missing'] = dataset[feature].apply(lambda f: 1 if f == '999' else 0)\n",
    "    dataset[feature] = imp.fit_transform(dataset[[feature]])\n",
    "\n",
    "print(zero_values)\n",
    "display(dataset['AFEC_DPTO_is_missing'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"datasets/dataset_clean.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Columns with zero values\n",
    "'''\n",
    "col_zero_values = set(dataset.columns[dataset.eq('0').mean() > 0])\n",
    "print(len(col_zero_values))\n",
    "print(col_zero_values)\n",
    "dataset = data_utils.impute_values(\"datasets/dataset_clean.csv\", \"datasets/dataset_clean_imputed.csv\")\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
